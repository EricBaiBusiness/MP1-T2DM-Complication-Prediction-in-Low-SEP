{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import storage\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/hakukazuho/Desktop/Keys/MP1_GCS_ServiceCount/EricBai's GCS Service Account Key.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a file from a bucket\n",
    "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "\n",
    "    print(f\"Downloaded {source_blob_name} from {bucket_name} to {destination_file_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploads a file to the bucket\n",
    "def upload_to_gcs(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"\n",
    "    Uploads a file to the bucket.\n",
    "    \n",
    "    Args:\n",
    "    bucket_name: Name of your GCS bucket.\n",
    "    source_file_name: Path to the file you want to upload.\n",
    "    destination_blob_name: Destination path in the bucket.\n",
    "    \"\"\"\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(f\"File {source_file_name} uploaded to {destination_blob_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded cleaned_data.csv from cdc_brfss_2023_test to cleaned_data.csv.\n"
     ]
    }
   ],
   "source": [
    "# Download setups\n",
    "bucket_name = 'cdc_brfss_2023_test'  # GCS bucket name\n",
    "source_blob_name = 'cleaned_data.csv'  # Path to your file in the GCS bucket\n",
    "destination_file_name = 'cleaned_data.csv'  # Local file name to save as\n",
    "\n",
    "download_blob(bucket_name, source_blob_name, destination_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File cleaned_data.csv uploaded to cleaned_data2.csv.\n"
     ]
    }
   ],
   "source": [
    "# Upload setups\n",
    "bucket_name = 'cdc_brfss_2023_test' # GCS bucket name\n",
    "source_file_name = 'cleaned_data.csv' # Local file name to upload\n",
    "destination_blob_name = 'cleaned_data2.csv' # Path to your file in the GCS bucket\n",
    "\n",
    "upload_to_gcs(bucket_name, source_file_name, destination_blob_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From https://github.com/EricBaiBusiness/MP1-T2DM-Complication-Prediction-in-Low-SEP\n",
      " * branch            Data-Cleaning -> FETCH_HEAD\n",
      "fatal: It seems that there is already a rebase-merge directory, and\n",
      "I wonder if you are in the middle of another rebase.  If that is the\n",
      "case, please try\n",
      "\tgit rebase (--continue | --abort | --skip)\n",
      "If that is not the case, please\n",
      "\trm -fr \".git/rebase-merge\"\n",
      "and run me again.  I am stopping in case you still have something\n",
      "valuable there.\n",
      "\n",
      "Enumerating objects: 22, done.\n",
      "Counting objects: 100% (22/22), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects: 100% (17/17), done.\n",
      "Writing objects: 100% (20/20), 65.80 MiB | 4.56 MiB/s, done.\n",
      "Total 20 (delta 6), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (6/6), completed with 1 local object.\u001b[K\n",
      "remote: \u001b[1;31merror\u001b[m: Trace: fa2e7473248f704a0a8c13d41526b444cc37e4a9bc1cd91778d53677615b504a\u001b[K\n",
      "remote: \u001b[1;31merror\u001b[m: See https://gh.io/lfs for more information.\u001b[K\n",
      "remote: \u001b[1;31merror\u001b[m: File data_2023.csv is 460.78 MB; this exceeds GitHub's file size limit of 100.00 MB\u001b[K\n",
      "remote: \u001b[1;31merror\u001b[m: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.\u001b[K\n",
      "To https://github.com/EricBaiBusiness/MP1-T2DM-Complication-Prediction-in-Low-SEP\n",
      " \u001b[31m! [remote rejected]\u001b[m Data-Cleaning -> Data-Cleaning (pre-receive hook declined)\n",
      "\u001b[31merror: failed to push some refs to 'https://github.com/EricBaiBusiness/MP1-T2DM-Complication-Prediction-in-Low-SEP'\n",
      "\u001b[m"
     ]
    }
   ],
   "source": [
    "# Solution to \"Try Pull First\" error\n",
    "!git pull --rebase origin Data-Cleaning\n",
    "!git push origin Data-Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git push origin Data-Cleaning --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git reset --hard <commit_id>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
